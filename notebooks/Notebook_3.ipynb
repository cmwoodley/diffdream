{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.autograd import Variable\n",
    "from diffusers import DDPMScheduler\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1,\"../scripts\")\n",
    "from get_voxels import get_mol_voxels, smile_to_sstring, collate_batch\n",
    "from networks import EncoderCNN, DecoderRNN, UNet3D, Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"models\" not in os.listdir(\"../\"):\n",
    "    os.mkdir(\"../models\")\n",
    "\n",
    "out_dir = \"../models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = []\n",
    "with open(\"../datasets/raw/zinc15_druglike_clean_canonical_max60.smi\") as f:\n",
    "    i=0\n",
    "    for i, line in enumerate(f):\n",
    "        smiles.append(line[:-1])\n",
    "        if i > 20000000:\n",
    "            break\n",
    "\n",
    "# smiles = smiles[112320:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, smiles):\n",
    "        self.smiles = smiles\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smile = self.smiles[idx]\n",
    "        return smile\n",
    "\n",
    "smile_DS = CustomImageDataset(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the networks\n",
    "encoderCNN = EncoderCNN(5)\n",
    "decoderRNN = DecoderRNN(512, 1024, 29, 1)\n",
    "encoder = Encoder()\n",
    "net = UNet3D(5,5)\n",
    "encoderCNN.to(device)\n",
    "decoderRNN.to(device)\n",
    "net.to(device)\n",
    "encoder.to(device)\n",
    "\n",
    "#Encoder Optimizer\n",
    "criterionEncoder = nn.BCELoss()\n",
    "#Encoder optimizer\n",
    "optimizerEncoder = torch.optim.Adam(encoder.parameters(), lr = 0.001)\n",
    "\n",
    "# Our loss finction\n",
    "criterionNet = nn.BCELoss()\n",
    "# The optimizer\n",
    "optimizerNet = torch.optim.Adam(net.parameters(), lr=0.001) \n",
    "\n",
    "# Caption optimizer\n",
    "criterionCaption = nn.CrossEntropyLoss()\n",
    "caption_params = list(decoderRNN.parameters()) + list(encoderCNN.parameters())\n",
    "caption_optimizer = torch.optim.Adam(caption_params, lr=0.001)\n",
    "\n",
    "# # How many runs through the data should we do?\n",
    "# n_epochs = 1\n",
    "\n",
    "#Other training stuff\n",
    "n_epochs = 1\n",
    "train_dataloader = DataLoader(smile_DS, batch_size=32, collate_fn=collate_batch)\n",
    "\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 of 8991.\n",
      "Batch 20 of 8991.\n",
      "Batch 30 of 8991.\n",
      "Batch 40 of 8991.\n",
      "Batch 50 of 8991.\n",
      "Batch 60 of 8991.\n",
      "Batch 70 of 8991.\n",
      "Batch 80 of 8991.\n",
      "Batch 90 of 8991.\n",
      "Batch 100 of 8991.\n",
      "Batch 110 of 8991.\n",
      "Batch 120 of 8991.\n",
      "Batch 130 of 8991.\n",
      "Batch 140 of 8991.\n",
      "Batch 150 of 8991.\n",
      "Batch 160 of 8991.\n",
      "Batch 170 of 8991.\n",
      "Batch 180 of 8991.\n",
      "Batch 190 of 8991.\n",
      "Batch 200 of 8991.\n",
      "Batch 210 of 8991.\n",
      "Batch 220 of 8991.\n",
      "Batch 230 of 8991.\n",
      "Batch 240 of 8991.\n",
      "Batch 250 of 8991.\n",
      "Batch 260 of 8991.\n",
      "Batch 270 of 8991.\n",
      "Batch 280 of 8991.\n",
      "Batch 290 of 8991.\n",
      "Batch 300 of 8991.\n",
      "Batch 310 of 8991.\n",
      "Batch 320 of 8991.\n",
      "Batch 330 of 8991.\n",
      "Batch 340 of 8991.\n",
      "Batch 350 of 8991.\n",
      "Batch 360 of 8991.\n",
      "Batch 370 of 8991.\n",
      "Batch 380 of 8991.\n",
      "Batch 390 of 8991.\n",
      "Batch 400 of 8991.\n",
      "Batch 410 of 8991.\n",
      "Batch 420 of 8991.\n",
      "Batch 430 of 8991.\n",
      "Batch 440 of 8991.\n",
      "Batch 450 of 8991.\n",
      "Batch 460 of 8991.\n",
      "Batch 470 of 8991.\n",
      "Batch 480 of 8991.\n",
      "Batch 490 of 8991.\n",
      "Batch 500 of 8991.\n",
      "Net Loss: 0.017858007922768593\n",
      "Encoder Loss: 0.00901155173778534\n",
      "Batch 510 of 8991.\n",
      "Batch 520 of 8991.\n",
      "Batch 530 of 8991.\n",
      "Batch 540 of 8991.\n",
      "Batch 550 of 8991.\n",
      "Batch 560 of 8991.\n",
      "Batch 570 of 8991.\n",
      "Batch 580 of 8991.\n",
      "Batch 590 of 8991.\n",
      "Batch 600 of 8991.\n",
      "Batch 610 of 8991.\n",
      "Batch 620 of 8991.\n",
      "Batch 630 of 8991.\n",
      "Batch 640 of 8991.\n",
      "Batch 650 of 8991.\n",
      "Batch 660 of 8991.\n",
      "Batch 670 of 8991.\n",
      "Batch 680 of 8991.\n",
      "Batch 690 of 8991.\n",
      "Batch 700 of 8991.\n",
      "Batch 710 of 8991.\n",
      "Batch 720 of 8991.\n",
      "Batch 730 of 8991.\n",
      "Batch 740 of 8991.\n",
      "Batch 750 of 8991.\n",
      "Batch 760 of 8991.\n",
      "Batch 770 of 8991.\n",
      "Batch 780 of 8991.\n",
      "Batch 790 of 8991.\n",
      "Batch 800 of 8991.\n",
      "Batch 810 of 8991.\n",
      "Batch 820 of 8991.\n",
      "Batch 830 of 8991.\n",
      "Batch 840 of 8991.\n",
      "Batch 850 of 8991.\n",
      "Batch 860 of 8991.\n",
      "Batch 870 of 8991.\n",
      "Batch 880 of 8991.\n",
      "Batch 890 of 8991.\n",
      "Batch 900 of 8991.\n",
      "Batch 910 of 8991.\n",
      "Batch 920 of 8991.\n",
      "Batch 930 of 8991.\n",
      "Batch 940 of 8991.\n",
      "Batch 950 of 8991.\n",
      "Batch 960 of 8991.\n",
      "Batch 970 of 8991.\n",
      "Batch 980 of 8991.\n",
      "Batch 990 of 8991.\n",
      "Batch 1000 of 8991.\n",
      "Net Loss: 0.013846945017576218\n",
      "Encoder Loss: 0.008552536368370056\n",
      "Batch 1010 of 8991.\n",
      "Batch 1020 of 8991.\n",
      "Batch 1030 of 8991.\n",
      "Batch 1040 of 8991.\n",
      "Batch 1050 of 8991.\n",
      "Batch 1060 of 8991.\n",
      "Batch 1070 of 8991.\n",
      "Batch 1080 of 8991.\n",
      "Batch 1090 of 8991.\n",
      "Batch 1100 of 8991.\n",
      "Batch 1110 of 8991.\n",
      "Batch 1120 of 8991.\n",
      "Batch 1130 of 8991.\n",
      "Batch 1140 of 8991.\n",
      "Batch 1150 of 8991.\n",
      "Batch 1160 of 8991.\n",
      "Gen_rep_fails\n",
      "Batch 1170 of 8991.\n"
     ]
    }
   ],
   "source": [
    "for i, (x, captions, pharm, lengths) in enumerate(train_dataloader):\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(\"Batch {} of {}.\".format(i+1,np.int64(np.ceil(len(train_dataloader.dataset)/train_dataloader.batch_size))))\n",
    "\n",
    "    #Train Encoder and Unet\n",
    "    ##Unet\n",
    "    timesteps = torch.randint(\n",
    "        0,\n",
    "        scheduler.num_train_timesteps,\n",
    "        (x.shape[0],),\n",
    "        device=x.device,\n",
    "    ).long()\n",
    "\n",
    "    noise = torch.randn(x.shape).to(x.device)\n",
    "    noisy_x = scheduler.add_noise(x, noise, timesteps)    \n",
    "    noisy_x = noisy_x.type(torch.FloatTensor).to(device)\n",
    "    x = x.to(device)\n",
    "\n",
    "    pred = net(noisy_x)\n",
    "    net_loss = criterionNet(pred, x)\n",
    "\n",
    "    # Backprop and update the params:\n",
    "    optimizerNet.zero_grad()\n",
    "    net_loss.backward()\n",
    "    optimizerNet.step()\n",
    "    net_loss = net_loss.cpu()\n",
    "\n",
    "    ##Encoder\n",
    "    pharm = pharm.to(device)\n",
    "    # Forward pass\n",
    "    encoded_tensor = encoder(x)\n",
    "    enc_loss=criterionEncoder(encoded_tensor, pharm)\n",
    "    # Backward and optimize\n",
    "    optimizerEncoder.zero_grad()\n",
    "    enc_loss.backward()\n",
    "    optimizerEncoder.step()\n",
    "\n",
    "\n",
    "    ##Train Captioning Networks after ~100,000 compounds \n",
    "    if i > 3200:\n",
    "        captions = Variable(captions.to(device))\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "        decoderRNN.zero_grad()\n",
    "        encoderCNN.zero_grad()\n",
    "        features = encoderCNN(pred.detach())\n",
    "        outputs = decoderRNN(features, captions, lengths)\n",
    "        cap_loss = criterionCaption(outputs, targets)\n",
    "        cap_loss.backward()\n",
    "        caption_optimizer.step()\n",
    "\n",
    "\n",
    "    if (i + 1) % 500 == 0:\n",
    "        torch.save(net.state_dict(),\"../models/net_weights_{}.pkl\".format(i+1))\n",
    "        torch.save(encoder.state_dict(),\"../models/net_weights_{}.pkl\".format(i+1))\n",
    "        torch.save(encoderCNN.state_dict(),\"../models/net_weights_{}.pkl\".format(i+1))\n",
    "        torch.save(decoderRNN.state_dict(),\"../models/net_weights_{}.pkl\".format(i+1))\n",
    "        if i > 3200:\n",
    "            print(\"Net Loss: {}\\nEncoder Loss: {}\\nCaptioning Loss: {}\".format(net_loss,enc_loss,cap_loss))\n",
    "        else:\n",
    "            print(\"Net Loss: {}\\nEncoder Loss: {}\".format(net_loss,enc_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ligdream",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "143b4da7727a9152bb94050db5642443df545a5cda503424792f6e55599a2504"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
